{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_genuine = 1000\n",
    "num_fraudulent = 50\n",
    "num_features = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_transactions = np.random.normal(0, 1, size=(num_genuine, num_features))\n",
    "fraudulent_transactions = np.random.normal(5, 2, size=(num_fraudulent, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a single dataset\n",
    "data = np.vstack([genuine_transactions, fraudulent_transactions])\n",
    "labels = np.hstack([np.zeros(num_genuine), np.ones(num_fraudulent)])\n",
    "\n",
    "print(\"Shape of data:\", data.shape)\n",
    "print(\"Shape of labels:\", labels.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(genuine_transactions[:, 0], genuine_transactions[:, 1], label='Genuine', alpha=0.7)\n",
    "plt.scatter(fraudulent_transactions[:, 0], fraudulent_transactions[:, 1], label='Fraudulent', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Dataset for Fraud Detection')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "latent_dim = 10\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "generator = Sequential([\n",
    "    Dense(15, activation='relu', input_dim=latent_dim),\n",
    "    Dense(num_features, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator model\n",
    "discriminator = Sequential([\n",
    "    Dense(15, activation='relu', input_dim=num_features),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "discriminator.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the GAN\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Generate fake samples\n",
    "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "    fake_data = generator.predict(noise)\n",
    "\n",
    "    # Select a random batch of real samples\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_data = X_train[idx]\n",
    "\n",
    "    # Combine fake and real samples\n",
    "    X = np.concatenate([real_data, fake_data])\n",
    "    y_dis = np.zeros(2 * batch_size)\n",
    "    y_dis[:batch_size] = 0.9  # label smoothing\n",
    "\n",
    "    # Train discriminator\n",
    "    discriminator.trainable = True\n",
    "    d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "    # Train generator (via the full GAN model, where the discriminator weights are frozen)\n",
    "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "    y_gen = np.ones(batch_size)\n",
    "    discriminator.trainable = False\n",
    "    g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}\")\n",
    "\n",
    "# Generate some synthetic fraud samples for visualization\n",
    "num_fraudulent = 1000\n",
    "noise = np.random.normal(0, 1, size=[num_fraudulent, latent_dim])\n",
    "generated_fraud = generator.predict(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming fraudulent_transactions is a subset of real fraud data for comparison\n",
    "fraudulent_transactions = X_train[Y_train == 1]\n",
    "\n",
    "# Plotting the generated fraud samples along with original fraud samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fraudulent_transactions[:, 0], fraudulent_transactions[:, 1], label='Original Fraudulent', alpha=0.7)\n",
    "plt.scatter(generated_fraud[:, 0], generated_fraud[:, 1], label='Generated Fraudulent', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Generated vs Original Fraudulent Transactions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
